{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import vader\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Since our real data is in the form of news article abstracts, we perform certain data cleaning steps:\n",
    "\n",
    "1) turn the entire sentence to lowercase\n",
    "2) handle negations: The way we are handling negation is by using the wordnet library which contains lists of antonyms for several words. Whenever the word \"not\" or \"n't\" is encountered, we replace the next word with its antonym. For example, \"not good\" becomes \"bad\". This process will allow us to then remove stopwords without affecting the sentiment of the sentence.\n",
    "3) remove punctuation\n",
    "4) remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-embedding model\n",
    "\n",
    "We'll start with a simple model which gives each word a sentiment score. The sentence sentiment will be determined by the *average* of the sentiment scores of the words in the sentence.\n",
    "\n",
    "For this model, we used nltk's vader sentiment analyzer to get the sentiment scores for each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data generation\n",
    "\n",
    "We generated synthetic data using the following process:\n",
    "1) obtain a vocabulary of words using nltk's sentiment analyzer's lexicon\n",
    "2) clean that vocabulary to only include words and filters out punctuation, numbers, etc.\n",
    "3) generate random sized sentences using random words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "sia = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "# make a vocabulary from the lexicon which excludes non alpha tokens\n",
    "vocab = sorted([token for token in sia.lexicon if token.isalpha()])\n",
    "\n",
    "values = np.array([sia.lexicon[word] for word in vocab])\n",
    "\n",
    "# show a histogram of vocab sentiment scores\n",
    "plt.hist(values, bins='auto')  # 'auto' automatically determines the number of bins\n",
    "plt.title('Histogram of VADER Lexicon Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
