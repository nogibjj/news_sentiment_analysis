{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import vader\n",
    "import numpy as np\n",
    "import prep_financial_phrasebank as prep # library for preprocessing dataset\n",
    "from afinn import Afinn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Phrasebank\n",
    "\n",
    "This dataset contains labelled sentences extracted from financial news. It's split into 4 categories, each one based on the % of experts that agreed on the sentence's label:\n",
    "1) sentences_50agree\n",
    "2) sentences_66agree\n",
    "3) sentences_75agree\n",
    "4) sentences_100agree\n",
    "\n",
    "The entire dataset contains 11,821 news articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Since our real data is in the form of news article abstracts, we perform certain data cleaning steps:\n",
    "\n",
    "1) turn the entire sentence to lowercase\n",
    "2) handle negations: The way we are handling negation is by using the wordnet library which contains lists of antonyms for several words. Whenever the word \"not\" or \"n't\" is encountered, we replace the next word with its antonym. For example, \"not good\" becomes \"bad\". This process will allow us to then remove stopwords without affecting the sentiment of the sentence.\n",
    "3) remove punctuation\n",
    "4) remove stopwords\n",
    "\n",
    "After the cleaning is done, we proceed with sentence tokenization. For tokenization, we'll use two approaches:\n",
    "1) single-embedding tokenization using the sentiment score for each word and **averaging** the sentiment scores for all words in a sentence\n",
    "2) using pretrained word2vec embeddings. This approach uses pretrained embeddings generated from the google news 300 dataset which was trained on 100 billion words and generates vectors of 300 dimensions for each word. In this approach we try to extract as much context as possible from each word in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-embedding tokenization\n",
    "\n",
    "We'll start with a simple model which gives each word a sentiment score. The sentence sentiment will be determined by the **average** of the sentiment scores of the words in the sentence.\n",
    "\n",
    "For this model, we used the score method from the afinn library to get the sentiment scores for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "afin = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_vader_scores(sentence):\n",
    "    sent_score = np.sum(np.array([afin.score(word) for word in sentence]))\n",
    "    return sent_score\n",
    "\n",
    "def generate_X_vader(df):\n",
    "    X = np.array([\n",
    "        sum_vader_scores(sentence)\n",
    "        for sentence in df['tokenized_sentences']\n",
    "    ])\n",
    "    return X.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean data\n",
    "fp = prep.import_all_splits()\n",
    "fp = prep.tokenize_financial_phrasebank(fp)\n",
    "# get X and y with afinn embeddings\n",
    "X_single = prep.generate_X_vader(fp)\n",
    "y_single = prep.generate_y_vader(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_single, y_single, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afinn (train): 0.6023688663282571\n",
      "afinn (test): 0.618064952638701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cerva\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "print(\"afinn (train):\", clf.score(X_train, y_train))\n",
    "print(\"afinn (test):\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (train): 0.6023688663282571\n",
      "word2vec (test): 0.618064952638701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cerva\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:424: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# prepare training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_single, y_single, test_size=0.2, random_state=42) \n",
    "\n",
    "gbc = prep.GradientBoost_experiment(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_single, y_single, test_size=0.2, random_state=42) \n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_torch = torch.from_numpy(X_train).float()\n",
    "y_train_torch = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Reshape X_train to be (batch_size, sequence_length, input_size)\n",
    "X_train_torch = X_train_torch.view(-1, 1, 1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleRNN(hidden_size=10)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_torch)\n",
    "    loss = criterion(outputs, y_train_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3628285527229309\n",
      "Accuracy: 0.6180649399757385\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_test_torch = torch.from_numpy(X_test).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Reshape X_test to be (batch_size, sequence_length, input_size)\n",
    "X_test_torch = X_test_torch.view(-1, 1, 1)\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_torch)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(predictions, y_test_torch)\n",
    "print(f'Test Loss: {loss.item()}')\n",
    "\n",
    "# Convert the model's output to binary labels\n",
    "predicted_labels = torch.round(predictions)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_predictions = (predicted_labels == y_test_torch).float().sum()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / y_test_torch.shape[0]\n",
    "\n",
    "print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "Accuracy:  0.6463620981387479\n"
     ]
    }
   ],
   "source": [
    "prep.RNN_experiment_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation\n",
    "\n",
    "We generated synthetic data using the following process:\n",
    "1) obtain a vocabulary of words using nltk's sentiment analyzer's lexicon\n",
    "2) clean that vocabulary to only include words and filters out punctuation, numbers, etc.\n",
    "3) generate random sized sentences using random words from the vocabulary\n",
    "4) create a label for each sentence by taking the **average** of the sentiment scores of the words in the sentence. \n",
    "\n",
    "The following histogram shows the distribution of the sentiment score in each of the words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "sia = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "# make a vocabulary from the lexicon which excludes non alpha tokens\n",
    "vocab = sorted([token for token in sia.lexicon if token.isalpha()])\n",
    "\n",
    "values = np.array([sia.lexicon[word] for word in vocab])\n",
    "\n",
    "# show a histogram of vocab sentiment scores\n",
    "plt.hist(values, bins='auto')  # 'auto' automatically determines the number of bins\n",
    "plt.title('Histogram of VADER Lexicon Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above distribution, we've decided to create a 'neutral' sentiment cutoff between [[ INPUT THE CUTOFFS WE CHOSE ]]. Aggregate scores below this range will be classified as 'negative' while scores above this range will be classified as 'positive'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
