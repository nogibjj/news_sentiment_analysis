{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence\n",
    "\n",
    "# import gensim\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from download_hf import download_parquet\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "\n",
    "\"\"\"\n",
    "This script was specifically written to prepare the financial_phrasebank dataset\n",
    "    financial_phrasebank has a single sentence per row\n",
    "    labels range from 0 (negative), 1(neutral), 2 (positive)\n",
    "    there are 4 datasets based on % agreement between annotators\n",
    "\"\"\"\n",
    "import gensim.downloader as api\n",
    "\n",
    "google_news = api.load(\"word2vec-google-news-300\")\n",
    "# google_news.save(\"word2vec-google-news-300.model\")\n",
    "\n",
    "\n",
    "# import sentiment_economy_news dataset\n",
    "def import_data(split):\n",
    "    \"\"\"\n",
    "    Import financial_phrasebank dataset\n",
    "    splits available:\n",
    "        sentences_50agree,\n",
    "        sentences_66agree,\n",
    "        sentences_75agree,\n",
    "        sentences_allagree\n",
    "    \"\"\"\n",
    "    return download_parquet(\"financial_phrasebank\", split)\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Negation(sentence):\n",
    "    \"\"\"\n",
    "    Input: Tokenized sentence (List of words)\n",
    "    Output: Tokenized sentence with negation handled (List of words)\n",
    "    \"\"\"\n",
    "    temp = int(0)\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i - 1] in [\"not\", \"n't\"]:\n",
    "            antonyms = []\n",
    "            for syn in wordnet.synsets(sentence[i]):\n",
    "                syns = wordnet.synsets(sentence[i])\n",
    "                w1 = syns[0].name()\n",
    "                temp = 0\n",
    "                for l in syn.lemmas():\n",
    "                    if l.antonyms():\n",
    "                        antonyms.append(l.antonyms()[0].name())\n",
    "                max_dissimilarity = 0\n",
    "                for ant in antonyms:\n",
    "                    syns = wordnet.synsets(ant)\n",
    "                    w2 = syns[0].name()\n",
    "                    syns = wordnet.synsets(sentence[i])\n",
    "                    w1 = syns[0].name()\n",
    "                    word1 = wordnet.synset(w1)\n",
    "                    word2 = wordnet.synset(w2)\n",
    "                    if isinstance(word1.wup_similarity(word2), float) or isinstance(\n",
    "                        word1.wup_similarity(word2), int\n",
    "                    ):\n",
    "                        temp = 1 - word1.wup_similarity(word2)\n",
    "                    if temp > max_dissimilarity:\n",
    "                        max_dissimilarity = temp\n",
    "                        antonym_max = ant\n",
    "                        sentence[i] = antonym_max\n",
    "                        sentence[i - 1] = \"\"\n",
    "    while \"\" in sentence:\n",
    "        sentence.remove(\"\")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text incorporating Negation handling and stopwords.\"\"\"\n",
    "    # turn to lowercase\n",
    "    text = text.lower()\n",
    "    # word tokenization\n",
    "    text = nltk.word_tokenize(text)\n",
    "    # negation handling\n",
    "    text = Negation(text)\n",
    "    # remove punctuation\n",
    "    text = [word for word in text if word.isalnum()]\n",
    "    # remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    if text == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        return \" \".join(text)\n",
    "\n",
    "\n",
    "def tokenize_financial_phrasebank(df):\n",
    "    \"\"\"\n",
    "    Tokenize sentiment economy news.\n",
    "    \"\"\"\n",
    "    # tokenize sentences\n",
    "    df[\"tokenized_sentences\"] = df[\"sentence\"].apply(clean_text)\n",
    "    df = df.loc[df.tokenized_sentences != \"\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def sum_token_embeddings(\n",
    "    token_embeddings: Sequence[FloatArray],\n",
    ") -> FloatArray:\n",
    "    \"\"\"Sum the token embeddings.\"\"\"\n",
    "    total: FloatArray = np.array(token_embeddings).sum(axis=0)\n",
    "    return total\n",
    "\n",
    "\n",
    "def map_labels(df):\n",
    "    \"\"\"Map labels to integers.\"\"\"\n",
    "    label_map = {0: -1, 1: 0, 2: 1}\n",
    "    return df[\"label\"].map(label_map)\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    X: FloatArray, y: FloatArray, test_percent: float = 20\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    N = len(y)\n",
    "    data_idx = list(range(N))\n",
    "    random.shuffle(data_idx)\n",
    "    break_idx = round(test_percent / 100 * N)\n",
    "    training_idx = data_idx[break_idx:]\n",
    "    testing_idx = data_idx[:break_idx]\n",
    "    X_train = X[training_idx, :]\n",
    "    y_train = y[training_idx]\n",
    "    X_test = X[testing_idx, :]\n",
    "    y_test = y[testing_idx]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_word2vec(df: pd.DataFrame) -> tuple[FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with word2vec.\"\"\"\n",
    "    # load pre-trained word2vec model\n",
    "    # google_news = gensim.models.KeyedVectors.load(\"word2vec-google-news-300.model\")\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            # sum the token embeddings for each sentence. If word is not in the model, return embedding of ['UNK']\n",
    "            sum_token_embeddings(\n",
    "                [\n",
    "                    google_news[word] if word in google_news else google_news[\"UNK\"]\n",
    "                    for _, word in enumerate(sentence)\n",
    "                ]\n",
    "            )\n",
    "            for _, sentence in enumerate(df.tokenized_sentences)\n",
    "        ]\n",
    "    )\n",
    "    # labels = [-1, 0, 1] seems to be causing an error\n",
    "    # y: FloatArray = np.array(map_labels(df))\n",
    "    y: FloatArray = np.array(df.label)\n",
    "    return split_train_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_observation_word2vec(sentence):\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            sum_token_embeddings(\n",
    "                [\n",
    "                    google_news[word] if word in google_news else google_news[\"UNK\"]\n",
    "                    for _, word in enumerate(sentence)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return X\n",
    "\n",
    "\n",
    "def etl(split):\n",
    "    \"\"\"\n",
    "    Extract, transform, and load financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = import_data(split)\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(df)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def aggregate_all_splits():\n",
    "    \"\"\"\n",
    "    Aggregate all splits of financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for split in [\n",
    "        \"sentences_50agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_allagree\",\n",
    "    ]:\n",
    "        df = pd.concat([df, import_data(split)])\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(df)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = aggregate_all_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_fin_data():\n",
    "    \"\"\"\n",
    "    Aggregate all splits of financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for split in [\n",
    "        \"sentences_50agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_allagree\",\n",
    "    ]:\n",
    "        df = pd.concat([df, import_data(split)])\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aggregate_fin_data()\n",
    "negative_df = df[df[\"label\"] == 0]\n",
    "neutral_df = df[df[\"label\"] == 1]\n",
    "positive_df = df[df[\"label\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2</td>\n",
       "      <td>new production plant company would increase ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>2</td>\n",
       "      <td>according company updated strategy years baswa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
       "      <td>2</td>\n",
       "      <td>financing aspocomp growth aspocomp aggressivel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "      <td>last quarter 2010 componenta net sales doubled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>2</td>\n",
       "      <td>third quarter 2010 net sales increased eur mn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>29 September , 2010 Finnish waste management a...</td>\n",
       "      <td>2</td>\n",
       "      <td>29 september 2010 finnish waste management rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>Key shareholders of Finnish IT services provid...</td>\n",
       "      <td>2</td>\n",
       "      <td>key shareholders finnish services provider tie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>As part of the transaction , M-real and Sappi ...</td>\n",
       "      <td>2</td>\n",
       "      <td>part transaction sappi also signed agreement s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>`` I am extremely delighted with this project ...</td>\n",
       "      <td>2</td>\n",
       "      <td>extremely delighted project continuation coope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>Danske Bank A-S DANSKE DC jumped 3.7 percent t...</td>\n",
       "      <td>2</td>\n",
       "      <td>danske bank danske dc jumped percent kroner re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "3     With the new production plant the company woul...      2   \n",
       "4     According to the company 's updated strategy f...      2   \n",
       "5     FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...      2   \n",
       "6     For the last quarter of 2010 , Componenta 's n...      2   \n",
       "7     In the third quarter of 2010 , net sales incre...      2   \n",
       "...                                                 ...    ...   \n",
       "2064  29 September , 2010 Finnish waste management a...      2   \n",
       "2074  Key shareholders of Finnish IT services provid...      2   \n",
       "2091  As part of the transaction , M-real and Sappi ...      2   \n",
       "2225  `` I am extremely delighted with this project ...      2   \n",
       "2229  Danske Bank A-S DANSKE DC jumped 3.7 percent t...      2   \n",
       "\n",
       "                                    tokenized_sentences  \n",
       "3     new production plant company would increase ca...  \n",
       "4     according company updated strategy years baswa...  \n",
       "5     financing aspocomp growth aspocomp aggressivel...  \n",
       "6     last quarter 2010 componenta net sales doubled...  \n",
       "7     third quarter 2010 net sales increased eur mn ...  \n",
       "...                                                 ...  \n",
       "2064  29 september 2010 finnish waste management rec...  \n",
       "2074  key shareholders finnish services provider tie...  \n",
       "2091  part transaction sappi also signed agreement s...  \n",
       "2225  extremely delighted project continuation coope...  \n",
       "2229  danske bank danske dc jumped percent kroner re...  \n",
       "\n",
       "[3988 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_string = \" \".join(positive_df[\"tokenized_sentences\"])\n",
    "positive_words = positive_string.split()\n",
    "\n",
    "neutral_string = \" \".join(neutral_df[\"tokenized_sentences\"])\n",
    "neutral_words = neutral_string.split()\n",
    "\n",
    "negative_string = \" \".join(negative_df[\"tokenized_sentences\"])\n",
    "negative_words = negative_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prep_financial_phrasebank as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"text\", \"label\"]\n",
    "synthetic_df = pd.DataFrame(columns=columns)\n",
    "for i in range(5000):\n",
    "    max_length = 15\n",
    "    length = int(np.round(np.random.uniform(0.15, 1) * max_length))\n",
    "    types = 2\n",
    "    sent = int(np.round(np.random.uniform(0, 1) * types))\n",
    "    if sent == 1:\n",
    "        rand_text = [random.choice(neutral_words) for _ in range(length)]\n",
    "    elif sent == 2:\n",
    "        rand_text = [random.choice(positive_words) for _ in range(length)]\n",
    "    elif sent == 0:\n",
    "        rand_text = [random.choice(negative_words) for _ in range(length)]\n",
    "    random_text = \" \".join(rand_text)\n",
    "    synthetic_df.loc[i] = [random_text, sent]  # Add rand_text to synthetic_df\n",
    "\n",
    "synthetic_df[\"text\"] = synthetic_df[\"text\"].apply(prep.clean_text)\n",
    "# remove empty strings\n",
    "synthetic_df = synthetic_df[synthetic_df[\"text\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_financial_phrasebank import (\n",
    "    tokenize_financial_phrasebank,\n",
    "    generate_data_word2vec,\n",
    "    sum_token_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_fake_splits(fakedata):\n",
    "    \"\"\"\n",
    "    Aggregate all splits of financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = fakedata\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(df)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def run_experiment1(synthetic_df) -> None:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # prepare training and testing data\n",
    "    X_train, y_train, X_test, y_test = aggregate_fake_splits(synthetic_df)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"word2vec (train):\", clf.score(X_train, y_train))\n",
    "    print(\"word2vec (test):\", clf.score(X_test, y_test))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flights passenger helsinki expected eur 500 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>make excluding fall totaled 22 published staff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>second arrive rival 2009 third impacted 2010 e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>area eur13 first volume march insurer eur732m ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010 oil projects houses 5 pm held thursday ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>price earliest decided</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>service refurbishment 2007 helsinki manufactur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>glaston registered works determining november ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>business outotec financial nordea approximatel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>created senior organizations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     flights passenger helsinki expected eur 500 co...      0\n",
       "1     make excluding fall totaled 22 published staff...      0\n",
       "2     second arrive rival 2009 third impacted 2010 e...      0\n",
       "3     area eur13 first volume march insurer eur732m ...      2\n",
       "4     2010 oil projects houses 5 pm held thursday ra...      1\n",
       "...                                                 ...    ...\n",
       "4995                             price earliest decided      1\n",
       "4996  service refurbishment 2007 helsinki manufactur...      1\n",
       "4997  glaston registered works determining november ...      1\n",
       "4998  business outotec financial nordea approximatel...      1\n",
       "4999                       created senior organizations      1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"sentence\", \"label\"]\n",
    "New_Synthetic_Data = pd.DataFrame(columns=columns)\n",
    "New_Synthetic_Data[\"sentence\"] = synthetic_df[\"text\"]\n",
    "New_Synthetic_Data[\"label\"] = synthetic_df[\"label\"]\n",
    "New_Synthetic_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_experiment_torch_synth(synthetic_df):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # prepare training and testing data\n",
    "    # X_train, y_train, X_test, y_test = aggregate_all_splits()\n",
    "    X_train, y_train, X_test, y_test = aggregate_fake_splits(synthetic_df)\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    # create dataset\n",
    "    class FinancialPhraseBankDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    # create dataloader\n",
    "    train_dataset = FinancialPhraseBankDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # define model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(300, 300)\n",
    "            self.fc2 = nn.Linear(300, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            # print(outputs.shape)\n",
    "            # print(labels.shape)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    # After training, generate predictions on test data\n",
    "    X_test = X_test.float()\n",
    "    outputs_test = net(X_test)\n",
    "    _, predicted_test = torch.max(outputs_test, 1)\n",
    "    outputs_train = net(X_train)\n",
    "    _, predicted_train = torch.max(outputs_train, 1)\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_test_np = y_test.numpy()\n",
    "    predicted_test_np = predicted_test.numpy()\n",
    "\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_train_np = y_train.numpy()\n",
    "    predicted_train_np = predicted_train.numpy()\n",
    "\n",
    "    # Now you can use sklearn's metrics to compare y_test_np and predicted_np\n",
    "    # For example, to calculate accuracy:\n",
    "    accuracy1 = metrics.accuracy_score(y_train_np, predicted_train_np)\n",
    "    print(\"Train Accuracy: \", accuracy1)\n",
    "    accuracy2 = metrics.accuracy_score(y_test_np, predicted_test_np)\n",
    "    print(\"Test Accuracy: \", accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adlerviton/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (train): 0.5895\n",
      "word2vec (test): 0.549\n",
      "[1,   100] loss: 1.093\n",
      "[2,   100] loss: 0.986\n",
      "[3,   100] loss: 0.954\n",
      "[4,   100] loss: 0.946\n",
      "[5,   100] loss: 0.943\n",
      "[6,   100] loss: 0.972\n",
      "[7,   100] loss: 0.937\n",
      "[8,   100] loss: 0.927\n",
      "[9,   100] loss: 0.952\n",
      "[10,   100] loss: 0.937\n",
      "Finished Training\n",
      "Train Accuracy:  0.55625\n",
      "Test Accuracy:  0.548\n"
     ]
    }
   ],
   "source": [
    "clf = run_experiment1(New_Synthetic_Data)\n",
    "rnn = RNN_experiment_torch_synth(New_Synthetic_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_experiment_torch():\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # prepare training and testing data\n",
    "    X_train, y_train, X_test, y_test = aggregate_all_splits()\n",
    "    # X_train, y_train, X_test, y_test = etl(\"sentences_allagree\")\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    # create dataset\n",
    "    class FinancialPhraseBankDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    # create dataloader\n",
    "    train_dataset = FinancialPhraseBankDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # define model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(300, 350)\n",
    "            self.fc2 = nn.Linear(350, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            # print(outputs.shape)\n",
    "            # print(labels.shape)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    # After training, generate predictions on test data\n",
    "    X_test = X_test.float()\n",
    "    outputs_test = net(X_test)\n",
    "    _, predicted_test = torch.max(outputs_test, 1)\n",
    "    outputs_train = net(X_train)\n",
    "    _, predicted_train = torch.max(outputs_train, 1)\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_test_np = y_test.numpy()\n",
    "    predicted_test_np = predicted_test.numpy()\n",
    "\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_train_np = y_train.numpy()\n",
    "    predicted_train_np = predicted_train.numpy()\n",
    "\n",
    "    # Now you can use sklearn's metrics to compare y_test_np and predicted_np\n",
    "    # For example, to calculate accuracy:\n",
    "    accuracy1 = metrics.accuracy_score(y_train_np, predicted_train_np)\n",
    "    print(\"Train Accuracy: \", accuracy1)\n",
    "    accuracy2 = metrics.accuracy_score(y_test_np, predicted_test_np)\n",
    "    print(\"Test Accuracy: \", accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_financial_phrasebank import run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adlerviton/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (train): 0.573\n",
      "word2vec (test): 0.605\n",
      "[1,   100] loss: 1.174\n",
      "[2,   100] loss: 0.974\n",
      "[3,   100] loss: 0.960\n",
      "[4,   100] loss: 0.972\n",
      "[5,   100] loss: 0.973\n",
      "[6,   100] loss: 0.945\n",
      "[7,   100] loss: 0.950\n",
      "[8,   100] loss: 0.953\n",
      "[9,   100] loss: 0.954\n",
      "[10,   100] loss: 0.945\n",
      "[11,   100] loss: 0.934\n",
      "[12,   100] loss: 0.929\n",
      "[13,   100] loss: 0.938\n",
      "[14,   100] loss: 0.932\n",
      "[15,   100] loss: 0.932\n",
      "[16,   100] loss: 0.945\n",
      "[17,   100] loss: 0.933\n",
      "[18,   100] loss: 0.918\n",
      "[19,   100] loss: 0.928\n",
      "[20,   100] loss: 0.943\n",
      "Finished Training\n",
      "Train Accuracy:  0.56975\n",
      "Test Accuracy:  0.585\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adlerviton/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (train): 0.6425852296760003\n",
      "word2vec (test): 0.6554991539763113\n",
      "[1,   100] loss: 1.168\n",
      "[1,   200] loss: 2.087\n",
      "[1,   300] loss: 3.004\n",
      "[2,   100] loss: 0.926\n",
      "[2,   200] loss: 1.823\n",
      "[2,   300] loss: 2.708\n",
      "[3,   100] loss: 0.907\n",
      "[3,   200] loss: 1.800\n",
      "[3,   300] loss: 2.691\n",
      "[4,   100] loss: 0.874\n",
      "[4,   200] loss: 1.760\n",
      "[4,   300] loss: 2.654\n",
      "[5,   100] loss: 0.890\n",
      "[5,   200] loss: 1.756\n",
      "[5,   300] loss: 2.634\n",
      "[6,   100] loss: 0.880\n",
      "[6,   200] loss: 1.748\n",
      "[6,   300] loss: 2.619\n",
      "[7,   100] loss: 0.878\n",
      "[7,   200] loss: 1.734\n",
      "[7,   300] loss: 2.601\n",
      "[8,   100] loss: 0.870\n",
      "[8,   200] loss: 1.724\n",
      "[8,   300] loss: 2.605\n",
      "[9,   100] loss: 0.865\n",
      "[9,   200] loss: 1.746\n",
      "[9,   300] loss: 2.615\n",
      "[10,   100] loss: 0.863\n",
      "[10,   200] loss: 1.764\n",
      "[10,   300] loss: 2.636\n",
      "[11,   100] loss: 0.881\n",
      "[11,   200] loss: 1.742\n",
      "[11,   300] loss: 2.619\n",
      "[12,   100] loss: 0.854\n",
      "[12,   200] loss: 1.715\n",
      "[12,   300] loss: 2.581\n",
      "[13,   100] loss: 0.854\n",
      "[13,   200] loss: 1.722\n",
      "[13,   300] loss: 2.569\n",
      "[14,   100] loss: 0.836\n",
      "[14,   200] loss: 1.695\n",
      "[14,   300] loss: 2.546\n",
      "[15,   100] loss: 0.847\n",
      "[15,   200] loss: 1.719\n",
      "[15,   300] loss: 2.559\n",
      "[16,   100] loss: 0.883\n",
      "[16,   200] loss: 1.752\n",
      "[16,   300] loss: 2.592\n",
      "[17,   100] loss: 0.839\n",
      "[17,   200] loss: 1.688\n",
      "[17,   300] loss: 2.540\n",
      "[18,   100] loss: 0.848\n",
      "[18,   200] loss: 1.693\n",
      "[18,   300] loss: 2.534\n",
      "[19,   100] loss: 0.849\n",
      "[19,   200] loss: 1.695\n",
      "[19,   300] loss: 2.524\n",
      "[20,   100] loss: 0.840\n",
      "[20,   200] loss: 1.692\n",
      "[20,   300] loss: 2.541\n",
      "Finished Training\n",
      "Train Accuracy:  0.6463074190000846\n",
      "Test Accuracy:  0.6517766497461929\n"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "clf = run_experiment1(New_Synthetic_Data)\n",
    "rnn = RNN_experiment_torch_synth(New_Synthetic_Data)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "logreg = run_experiment()\n",
    "rnn2 = RNN_experiment_torch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
