{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence\n",
    "\n",
    "# import gensim\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from download_hf import download_parquet\n",
    "\n",
    "from afinn import Afinn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "\n",
    "\"\"\"\n",
    "This script was specifically written to prepare the financial_phrasebank dataset\n",
    "    financial_phrasebank has a single sentence per row\n",
    "    labels range from 0 (negative), 1(neutral), 2 (positive)\n",
    "    there are 4 datasets based on % agreement between annotators\n",
    "\"\"\"\n",
    "import gensim.downloader as api\n",
    "\n",
    "google_news = api.load(\"word2vec-google-news-300\")\n",
    "# google_news.save(\"word2vec-google-news-300.model\")\n",
    "afin = Afinn()\n",
    "\n",
    "\n",
    "# import sentiment_economy_news dataset\n",
    "def import_data(split):\n",
    "    \"\"\"\n",
    "    Import financial_phrasebank dataset\n",
    "    splits available:\n",
    "        sentences_50agree,\n",
    "        sentences_66agree,\n",
    "        sentences_75agree,\n",
    "        sentences_allagree\n",
    "    \"\"\"\n",
    "    return download_parquet(\"financial_phrasebank\", split)\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Define a bunch of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Negation(sentence):\n",
    "    \"\"\"\n",
    "    Input: Tokenized sentence (List of words)\n",
    "    Output: Tokenized sentence with negation handled (List of words)\n",
    "    \"\"\"\n",
    "    temp = int(0)\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i - 1] in [\"not\", \"n't\"]:\n",
    "            antonyms = []\n",
    "            for syn in wordnet.synsets(sentence[i]):\n",
    "                syns = wordnet.synsets(sentence[i])\n",
    "                w1 = syns[0].name()\n",
    "                temp = 0\n",
    "                for l in syn.lemmas():\n",
    "                    if l.antonyms():\n",
    "                        antonyms.append(l.antonyms()[0].name())\n",
    "                max_dissimilarity = 0\n",
    "                for ant in antonyms:\n",
    "                    syns = wordnet.synsets(ant)\n",
    "                    w2 = syns[0].name()\n",
    "                    syns = wordnet.synsets(sentence[i])\n",
    "                    w1 = syns[0].name()\n",
    "                    word1 = wordnet.synset(w1)\n",
    "                    word2 = wordnet.synset(w2)\n",
    "                    if isinstance(word1.wup_similarity(word2), float) or isinstance(\n",
    "                        word1.wup_similarity(word2), int\n",
    "                    ):\n",
    "                        temp = 1 - word1.wup_similarity(word2)\n",
    "                    if temp > max_dissimilarity:\n",
    "                        max_dissimilarity = temp\n",
    "                        antonym_max = ant\n",
    "                        sentence[i] = antonym_max\n",
    "                        sentence[i - 1] = \"\"\n",
    "    while \"\" in sentence:\n",
    "        sentence.remove(\"\")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text incorporating Negation handling and stopwords.\"\"\"\n",
    "    # turn to lowercase\n",
    "    text = text.lower()\n",
    "    # word tokenization\n",
    "    text = nltk.word_tokenize(text)\n",
    "    # negation handling\n",
    "    text = Negation(text)\n",
    "    # remove punctuation\n",
    "    text = [word for word in text if word.isalnum()]\n",
    "    # remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    if text == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        return \" \".join(text)\n",
    "\n",
    "\n",
    "def tokenize_financial_phrasebank(df):\n",
    "    \"\"\"\n",
    "    Tokenize sentiment economy news.\n",
    "    \"\"\"\n",
    "    # tokenize sentences\n",
    "    df[\"tokenized_sentences\"] = df[\"sentence\"].apply(clean_text)\n",
    "    df = df.loc[df.tokenized_sentences != \"\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def sum_token_embeddings(\n",
    "    token_embeddings: Sequence[FloatArray],\n",
    ") -> FloatArray:\n",
    "    \"\"\"Sum the token embeddings.\"\"\"\n",
    "    total: FloatArray = np.array(token_embeddings).sum(axis=0)\n",
    "    return total\n",
    "\n",
    "\n",
    "def map_labels(df):\n",
    "    \"\"\"Map labels to integers.\"\"\"\n",
    "    label_map = {0: -1, 1: 0, 2: 1}\n",
    "    return df[\"label\"].map(label_map)\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    X: FloatArray, y: FloatArray, test_percent: float = 20\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    N = len(y)\n",
    "    data_idx = list(range(N))\n",
    "    random.shuffle(data_idx)\n",
    "    break_idx = round(test_percent / 100 * N)\n",
    "    training_idx = data_idx[break_idx:]\n",
    "    testing_idx = data_idx[:break_idx]\n",
    "    X_train = X[training_idx, :]\n",
    "    y_train = y[training_idx]\n",
    "    X_test = X[testing_idx, :]\n",
    "    y_test = y[testing_idx]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_word2vec(df: pd.DataFrame) -> tuple[FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with word2vec.\"\"\"\n",
    "    # load pre-trained word2vec model\n",
    "    # google_news = gensim.models.KeyedVectors.load(\"word2vec-google-news-300.model\")\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            # sum the token embeddings for each sentence. If word is not in the model, return embedding of ['UNK']\n",
    "            sum_token_embeddings(\n",
    "                [\n",
    "                    google_news[word] if word in google_news else google_news[\"UNK\"]\n",
    "                    for _, word in enumerate(sentence)\n",
    "                ]\n",
    "            )\n",
    "            for _, sentence in enumerate(df.tokenized_sentences)\n",
    "        ]\n",
    "    )\n",
    "    # labels = [-1, 0, 1] seems to be causing an error\n",
    "    # y: FloatArray = np.array(map_labels(df))\n",
    "    y: FloatArray = np.array(df.label)\n",
    "    return split_train_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_observation_word2vec(sentence):\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            sum_token_embeddings(\n",
    "                [\n",
    "                    google_news[word] if word in google_news else google_news[\"UNK\"]\n",
    "                    for _, word in enumerate(sentence)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return X\n",
    "\n",
    "\n",
    "def etl(split):\n",
    "    \"\"\"\n",
    "    Extract, transform, and load financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = import_data(split)\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(df)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def aggregate_all_splits():\n",
    "    \"\"\"\n",
    "    Aggregate all splits of financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for split in [\n",
    "        \"sentences_50agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_allagree\",\n",
    "    ]:\n",
    "        df = pd.concat([df, import_data(split)])\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(df)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_fin_data():\n",
    "    \"\"\"\n",
    "    Aggregate all splits of financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for split in [\n",
    "        \"sentences_50agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_allagree\",\n",
    "    ]:\n",
    "        df = pd.concat([df, import_data(split)])\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Create Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aggregate_fin_data()\n",
    "negative_df = df[df[\"label\"] == 0]\n",
    "neutral_df = df[df[\"label\"] == 1]\n",
    "positive_df = df[df[\"label\"] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Create lists of all the words in each sentiment category (including redundancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_string = \" \".join(positive_df[\"tokenized_sentences\"])\n",
    "positive_words = positive_string.split()\n",
    "\n",
    "neutral_string = \" \".join(neutral_df[\"tokenized_sentences\"])\n",
    "neutral_words = neutral_string.split()\n",
    "\n",
    "negative_string = \" \".join(negative_df[\"tokenized_sentences\"])\n",
    "negative_words = negative_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prep_financial_phrasebank as prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Make a Dataframe where each row is a random probabilistic collection of words from that sentiment's associated word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"text\", \"label\"]\n",
    "synthetic_df = pd.DataFrame(columns=columns)\n",
    "for i in range(5000):\n",
    "    max_length = 15\n",
    "    length = int(np.round(np.random.uniform(0.15, 1) * max_length))\n",
    "    types = 2\n",
    "    sent = int(np.round(np.random.uniform(0, 1) * types))\n",
    "    if sent == 1:\n",
    "        rand_text = [random.choice(neutral_words) for _ in range(length)]\n",
    "    elif sent == 2:\n",
    "        rand_text = [random.choice(positive_words) for _ in range(length)]\n",
    "    elif sent == 0:\n",
    "        rand_text = [random.choice(negative_words) for _ in range(length)]\n",
    "    random_text = \" \".join(rand_text)\n",
    "    synthetic_df.loc[i] = [random_text, sent]  # Add rand_text to synthetic_df\n",
    "\n",
    "synthetic_df[\"text\"] = synthetic_df[\"text\"].apply(prep.clean_text)\n",
    "# remove empty strings\n",
    "synthetic_df = synthetic_df[synthetic_df[\"text\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_financial_phrasebank import (\n",
    "    tokenize_financial_phrasebank,\n",
    "    generate_data_word2vec,\n",
    "    sum_token_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Train 2 1D Afin models on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_vader_scores(sentence):\n",
    "    sent_score = np.sum(np.array([afin.score(word) for word in sentence]))\n",
    "    return sent_score\n",
    "\n",
    "\n",
    "def generate_X_vader(df):\n",
    "    X = np.array([sum_vader_scores(sentence) for sentence in df[\"text\"]])\n",
    "    return X.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def generate_y_vader(df):\n",
    "    y = np.array(df.label).reshape(-1, 1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>investments absolut nomination 51 expected 200...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drying investrend new part panfish software re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>systems printing ponzi financially eur0 2009 n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>due prospects eur first slide construction pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>increased result corresponding solutions serie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>salaried also customers rental region</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>entitle scheduled continued company customized...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>earlier said systems alone lassila decision ea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>profit finnish eur eur320 chief mn unit per ra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>recruitment requisition price stake korean rat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     investments absolut nomination 51 expected 200...      1\n",
       "1     drying investrend new part panfish software re...      1\n",
       "2     systems printing ponzi financially eur0 2009 n...      0\n",
       "3     due prospects eur first slide construction pro...      0\n",
       "4     increased result corresponding solutions serie...      2\n",
       "...                                                 ...    ...\n",
       "4995              salaried also customers rental region      1\n",
       "4996  entitle scheduled continued company customized...      1\n",
       "4997  earlier said systems alone lassila decision ea...      0\n",
       "4998  profit finnish eur eur320 chief mn unit per ra...      0\n",
       "4999  recruitment requisition price stake korean rat...      1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Train afinn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afinn (train): 0.501\n",
      "afinn (test): 0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adlerviton/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_single = generate_X_vader(synthetic_df)\n",
    "y_single = generate_y_vader(synthetic_df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_single, y_single, test_size=0.2, random_state=42\n",
    ")\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"afinn Logistic Regression (train):\", clf.score(X_train, y_train))\n",
    "print(\"afinn Logistic Regression (test):\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Train afinn RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_single, y_single, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_torch = torch.from_numpy(X_train).float()\n",
    "y_train_torch = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Reshape X_train to be (batch_size, sequence_length, input_size)\n",
    "X_train_torch = X_train_torch.view(-1, 1, 1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleRNN(hidden_size=10)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_torch)\n",
    "    loss = criterion(outputs, y_train_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4890991747379303\n",
      "afinn Logistic Regression (train): 0.501\n",
      "afinn Logistic Regression (test): 0.511\n",
      "afinn RNN (train)): 0.5009999871253967\n",
      "afinn RNN (test)): 0.5109999775886536\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_test_torch = torch.from_numpy(X_test).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "X_train_torch = torch.from_numpy(X_train).float()\n",
    "y_train_torch = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Reshape X_test to be (batch_size, sequence_length, input_size)\n",
    "X_test_torch = X_test_torch.view(-1, 1, 1)\n",
    "X_train_torch = X_train_torch.view(-1, 1, 1)\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions_test = model(X_test_torch)\n",
    "    predictions_train = model(X_train_torch)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(predictions_test, y_test_torch)\n",
    "print(f\"Test Loss: {loss.item()}\")\n",
    "\n",
    "# Convert the model's output to binary labels\n",
    "predicted_test_labels = torch.round(predictions_test)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_test_predictions = (predicted_test_labels == y_test_torch).float().sum()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_test = correct_test_predictions / y_test_torch.shape[0]\n",
    "\n",
    "# Convert the model's output to binary labels\n",
    "predicted_train_labels = torch.round(predictions_train)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_train_predictions = (predicted_train_labels == y_train_torch).float().sum()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_train = correct_train_predictions / y_train_torch.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Get Results for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afinn Logistic Regression (train): 0.501\n",
      "afinn Logistic Regression (test): 0.511\n",
      "afinn RNN (train)): 0.5009999871253967\n",
      "afinn RNN (test)): 0.5109999775886536\n"
     ]
    }
   ],
   "source": [
    "print(\"afinn Logistic Regression (train):\", clf.score(X_train, y_train))\n",
    "print(\"afinn Logistic Regression (test):\", clf.score(X_test, y_test))\n",
    "print(f\"afinn RNN (train)): {accuracy_train.item()}\")\n",
    "print(f\"afinn RNN (test)): {accuracy_test.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Train 2 Word2Vec models on both synthetic and real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. make functions to run Logistic Regression Model on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_fake_splits(fakedata):\n",
    "    \"\"\"\n",
    "    Aggregate all splits of financial_phrasebank\n",
    "    \"\"\"\n",
    "    df = fakedata\n",
    "    df = tokenize_financial_phrasebank(df)\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(df)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def run_experiment1(synthetic_df) -> None:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # prepare training and testing data\n",
    "    X_train, y_train, X_test, y_test = aggregate_fake_splits(synthetic_df)\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"word2vec (train):\", clf.score(X_train, y_train))\n",
    "    print(\"word2vec (test):\", clf.score(X_test, y_test))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>investments absolut nomination 51 expected 200...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drying investrend new part panfish software re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>systems printing ponzi financially eur0 2009 n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>due prospects eur first slide construction pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>increased result corresponding solutions serie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>salaried also customers rental region</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>entitle scheduled continued company customized...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>earlier said systems alone lassila decision ea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>profit finnish eur eur320 chief mn unit per ra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>recruitment requisition price stake korean rat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     investments absolut nomination 51 expected 200...      1\n",
       "1     drying investrend new part panfish software re...      1\n",
       "2     systems printing ponzi financially eur0 2009 n...      0\n",
       "3     due prospects eur first slide construction pro...      0\n",
       "4     increased result corresponding solutions serie...      2\n",
       "...                                                 ...    ...\n",
       "4995              salaried also customers rental region      1\n",
       "4996  entitle scheduled continued company customized...      1\n",
       "4997  earlier said systems alone lassila decision ea...      0\n",
       "4998  profit finnish eur eur320 chief mn unit per ra...      0\n",
       "4999  recruitment requisition price stake korean rat...      1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"sentence\", \"label\"]\n",
    "New_Synthetic_Data = pd.DataFrame(columns=columns)\n",
    "New_Synthetic_Data[\"sentence\"] = synthetic_df[\"text\"]\n",
    "New_Synthetic_Data[\"label\"] = synthetic_df[\"label\"]\n",
    "New_Synthetic_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define RNN model for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_experiment_torch_synth(synthetic_df):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # prepare training and testing data\n",
    "    # X_train, y_train, X_test, y_test = aggregate_all_splits()\n",
    "    X_train, y_train, X_test, y_test = aggregate_fake_splits(synthetic_df)\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    # create dataset\n",
    "    class FinancialPhraseBankDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    # create dataloader\n",
    "    train_dataset = FinancialPhraseBankDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # define model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(300, 300)\n",
    "            self.fc2 = nn.Linear(300, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            # print(outputs.shape)\n",
    "            # print(labels.shape)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            # if i % 100 == 99:  # print every 100 mini-batches\n",
    "            # print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "    # print(\"Finished Training\")\n",
    "    # After training, generate predictions on test data\n",
    "    X_test = X_test.float()\n",
    "    outputs_test = net(X_test)\n",
    "    _, predicted_test = torch.max(outputs_test, 1)\n",
    "    outputs_train = net(X_train)\n",
    "    _, predicted_train = torch.max(outputs_train, 1)\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_test_np = y_test.numpy()\n",
    "    predicted_test_np = predicted_test.numpy()\n",
    "\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_train_np = y_train.numpy()\n",
    "    predicted_train_np = predicted_train.numpy()\n",
    "\n",
    "    # Now you can use sklearn's metrics to compare y_test_np and predicted_np\n",
    "    # For example, to calculate accuracy:\n",
    "    accuracy1 = metrics.accuracy_score(y_train_np, predicted_train_np)\n",
    "    print(\"RNN (train): \", accuracy1)\n",
    "    accuracy2 = metrics.accuracy_score(y_test_np, predicted_test_np)\n",
    "    print(\"RNN (test): \", accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Define RNN model for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_experiment_torch():\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # prepare training and testing data\n",
    "    X_train, y_train, X_test, y_test = aggregate_all_splits()\n",
    "    # X_train, y_train, X_test, y_test = etl(\"sentences_allagree\")\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    # create dataset\n",
    "    class FinancialPhraseBankDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    # create dataloader\n",
    "    train_dataset = FinancialPhraseBankDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # define model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(300, 350)\n",
    "            self.fc2 = nn.Linear(350, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            # print(outputs.shape)\n",
    "            # print(labels.shape)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            # if i % 100 == 99:  # print every 100 mini-batches\n",
    "            # print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "    # print(\"Finished Training\")\n",
    "    # After training, generate predictions on test data\n",
    "    X_test = X_test.float()\n",
    "    outputs_test = net(X_test)\n",
    "    _, predicted_test = torch.max(outputs_test, 1)\n",
    "    outputs_train = net(X_train)\n",
    "    _, predicted_train = torch.max(outputs_train, 1)\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_test_np = y_test.numpy()\n",
    "    predicted_test_np = predicted_test.numpy()\n",
    "\n",
    "    # Convert tensors to numpy arrays for comparison with sklearn metrics\n",
    "    y_train_np = y_train.numpy()\n",
    "    predicted_train_np = predicted_train.numpy()\n",
    "\n",
    "    # Now you can use sklearn's metrics to compare y_test_np and predicted_np\n",
    "    # For example, to calculate accuracy:\n",
    "    accuracy1 = metrics.accuracy_score(y_train_np, predicted_train_np)\n",
    "    print(\"RNN (train): \", accuracy1)\n",
    "    accuracy2 = metrics.accuracy_score(y_test_np, predicted_test_np)\n",
    "    print(\"RNN (test): \", accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Get logistic model for real datafrom prep_financial_phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_financial_phrasebank import run_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results from both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adlerviton/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (train): 0.5805\n",
      "word2vec (test): 0.585\n",
      "RNN (train):  0.56\n",
      "RNN (test):  0.573\n",
      "\n",
      "\n",
      "\n",
      "Real Data Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adlerviton/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (train): 0.6507909652313679\n",
      "word2vec (test): 0.6385786802030456\n",
      "RNN (train):  0.6434311817951104\n",
      "RNN (test):  0.6490693739424704\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic Data Results:\")\n",
    "clf = run_experiment1(New_Synthetic_Data)\n",
    "rnn = RNN_experiment_torch_synth(New_Synthetic_Data)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Real Data Results:\")\n",
    "logreg = run_experiment()\n",
    "rnn2 = RNN_experiment_torch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
